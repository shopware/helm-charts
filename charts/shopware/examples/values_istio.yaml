# This is more off an advanced configuration and is just an example.
useIstio: true
region: eu-central-1

percona:
  # You can disable percona and use your own database. Just modify the storage database
  # part in this file.
  # TODO: Not tested yet, database part is not variable
  enabled: true

  # The version for percona to use. This is also the image version for the percona-xtradb-cluster
  version: 1.14.0
  # Allows only one database and allows to overwrite the topologySpreadConstraints
  allowUnsafeConfigurations: false

  database:
    version: "8.0"
    # These are the default values for the data
    size: 3

    # You can overwrite these default values from percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a pr.
    resources:
      requests:
        memory: 4Gi
        cpu: 1000m
      limits:
        memory: 8Gi
        cpu: 1200m
    annotations:
      sidecar.istio.io/inject: "false"
    # affinity:
    serviceAnnotations:
      sidecar.istio.io/inject: "false"
    storageClassName: gp3
    storageSize: 50Gi

    # We overwrite the topologySpreadConstraints for this to schedule only one database
    topologySpreadConstraints:
    # Currently percona only supports amd64, so we set a node seletor. This could also be
    # a problem for our mac users here.
    # TODO: Change the image if they support amd and arm architectures
    nodeSelector:
      kubernetes.io/arch: amd64
      karpenter.k8s.aws/instance-generation: '7'

  # This is only useful if you want more performance since the sql-proxy is using connection-pooling.
  proxy:
    enabled: true

    # You can overwrite these default values from percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a pr.
    # size: 1
    # annotations:
    resources:
      requests:
        memory: 1Gi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1500m
    # topologySpreadConstraints:
    # affinity:
    annotations:
      sidecar.istio.io/inject: "false"
    serviceAnnotations:
      "sidecar.istio.io/inject": "false"
    storageClassName: gp2
    storageSize: 10G

    # Currently percona only supports amd64, so we set a node seletor. This could also be
    # a problem for our mac users here.
    # TODO: Change the image if they support amd and arm architectures
    nodeSelector:
      kubernetes.io/arch: amd64

store:
  # We use traefik.me to resolve to 127.0.0.1
  # you can change to any ip address if you want to change it.
  host: localhost.traefik.me

  # blackfire:
  #   enabled: false
  #   host: blackfire
  #   port: 8707

  otel:
    enabled: true
    exporterEndpoint: http://opentelemetry-collector.opentelemetry-collector.svc.cluster.local:4317

  container:
    # This image is used for the setup job and the deployment. If you want to migrate to a new
    # version just update this and the operator makes sure to handle the migrations.
    # TODO: link to shopware operator
    image: ghcr.io/shopware-redstone/customer:76f31d75df7e18b090856fff8ef0a646c4197cc6
    # imagePullPolicy: IfNotPresent
    # restartPolicy: Always
    replicas: 2
    # progressDeadlineSeconds: 30
    # topologySpreadConstraints:
    # affinity:
    # nodeSelector:
    imagePullSecrets:
      - name: regcred
    resources:
      requests:
        memory: 2Gi
        cpu: 1000m
      limits:
        memory: 2Gi
        cpu: 1500m

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  network:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/service-upstream: "true"

  # You can run commands before and after the /setup is executed.
  setupHook:
    after: curl -fsI -X POST http://localhost:15020/quitquitquit

  horizontalPodAutoscaler:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    behavior:
      scaleDown:
        policies:
          - type: Pods
            value: 1
            periodSeconds: 60
    metrics:
      - type: Pods
        pods:
          metric:
            name: phpfpm_process_utilization
          target:
            averageValue: "50"
            type: AverageValue

minio:
  # This enabled the s3 minio and creates a Tenant. This requires the minio operator
  # to work properly. You can use aws s3 also, but then need to overwrite the values
  # in the store resource.
  enabled: true
  # This disable the minio mtls. Please don't use this in production! This is required
  # for a local or Istio installation.
  useTLS: false
  # This enabled readonly for anonymoud on the shopware-public bucket. This is required
  # for shopware to work properly. But you can disable it. This will trigger a job in
  # kubernetes.
  publishPublicBucket: true
  certSecretName: tenant-cert
  secretName: minio-s3
  tenantVersion: quay.io/minio/minio:RELEASE.2024-04-06T05-26-02Z
  storageClassName: gp3
  storageSize: 10Gi

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  ingress:
    enabled: true
    className: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      # Shopware addmin app.js is to big for the default more then 64MB
      nginx.ingress.kubernetes.io/proxy-body-size: '0'
      nginx.ingress.kubernetes.io/service-upstream: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"

# This is used by shopware to map the aws secret into our cluster to pull images
regcred:
  clusterStore:
    name: aws-secrets
  aws:
    name: github-regcred

opensearch:
  enabled: false

redissession:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisapp:
  enabled: true
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisqueue:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 2000m
  auth:
    enabled: false

blackfire:
  image: blackfire/blackfire:latest
  serverID: id
  serverToken: token
  # port: 8307
  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 100m

# Percona has currently only amd images, so we make sure with a mixed cluster that we
# use the amd clusters.
pxc-operator:
  nodeSelector:
    kubernetes.io/arch: amd64

# This controls the shopware operator helm chart which can be found here:
# TODO: Helm chart link
shopware-operator:
  # Disableing makes sense if you want to test the Operator itself
  enabled: false

grafana:
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://shopware-prometheus-server.shopware-redstone-staging.svc.cluster.local
        - name: Loki
          type: loki
          url: http://loki-gateway.loki.svc.cluster.local

prometheus:
  prometheus-node-exporter:
    enabled: false

promtail:
  daemonset:
    enabled: false
  deployment:
    enabled: true
  config:
    clients:
      - url: http://loki-gateway.loki.svc.cluster.local/loki/api/v1/push
  server:
    remoteWrite:
      - url: http://mimir-nginx.mimir.svc:80/api/v1/push
