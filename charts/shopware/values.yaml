# This is for more production and enterprise solution. To see a full
# example see `examples/values_istio.yaml`
#useIstio: false

percona:
  # You can disable Percona and use your own database. Just modify the storage database
  # part in this file.
  # TODO: Not tested yet, database part is not variable
  enabled: false

  # The version for Percona to use. This is also the image version for the percona-xtradb-cluster
  version: 1.14.0
  # Allows only one database and allows to overwrite the topologySpreadConstraints
  allowUnsafeConfigurations: true

  database:
    version: "8.0"
    # These are the default values for the data
    # size: 1

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 2000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 2000m
    # annotations:
    # affinity:
    # serviceAnnotations:
    # storageClassName: standard
    # storageSize: 8Gi

    # We overwrite the topologySpreadConstraints for this to schedule only one database
    topologySpreadConstraints:
    # Currently Percona only supports amd64, so we set a node selector.
    # This could also be a problem for Mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

  # This is only useful if you want more performance since the sql-proxy is using connection-pooling.
  proxy:
    enabled: false

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # size: 1
    # annotations:
    # resources:
    #   requests:
    #     memory: 1Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 1Gi
    #     cpu: 1000m
    # topologySpreadConstraints:
    # affinity:
    # annotations:
    # serviceAnnotations:
    # storageClassName: standard
    # storageSize: 10Gi

    # Currently Percona only supports amd64, so we set a node selector. This could also be
    # a problem for our mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

store:
  # We use traefik.me to resolve to 127.0.0.1
  # Enter your DNS name for the shop.
  # This will make sure that your sales-channel is set up correctly and the ingress is using the host address.
  host: localhost.traefik.me

  # blackfire:
  #   enabled: false
  #   host: blackfire
  #   port: 8707

  # If you want to use tracing with otel make sure to disable blackfire.
  # It is not possible to use both.
  # otel:
  #   enabled: false
  #   exporterEndpoint: http://opentelemetry-collector.opentelemetry-collector.svc.cluster.local:4317

  container:
    # This image is used for the setup job and the deployment. If you want to migrate to a new
    # version just update this and the operator makes sure to handle the migrations.
    # TODO: link to Shopware operator
    image: ghcr.io/shopware-redstone/customer:76f31d75df7e18b090856fff8ef0a646c4197cc6
    # imagePullPolicy: IfNotPresent
    # restartPolicy: Always
    # replicas: 2
    # progressDeadlineSeconds: 30
    # topologySpreadConstraints:
    # affinity:
    # nodeSelector:
    # imagePullSecrets:
    #   - name: regcred
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 1000m
    #

    # You can add side-car containers as you wish. Note that the PHP fpm exporter is
    # already attached, if you are not using `dynamic` in the fpm configuration.
    # extraContainers:
    #   - name: some-side-pod
    #     env:
    #     - name: PHP_FPM_SCRAPE_URI
    #       value: tcp://127.0.0.1:9000/status
    #     image: hipages/php-fpm_exporter
    #     imagePullPolicy: IfNotPresent
    #     ports:
    #     - containerPort: 9253
    #       protocol: TCP
    #     resources:
    #       limits:
    #         cpu: 30m
    #         memory: 32Mi
    #       requests:
    #         cpu: 10m
    #         memory: 10Mi
    #     terminationMessagePath: /dev/termination-log
    #     terminationMessagePolicy: File

    # Warning: You can overwrite ENVs provided by the operator. If a variable gets overwritten
    # The operator prints a warning in the logs.
    # extraEnvs:
    #   - name: ENVS
    #     value: SomeStringValue
    #   - name: FPM_PM
    #     value: static

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  network:
    enabled: true
    ingressClassName: nginx
    # annotations:

  # You can run commands before and after the /setup is executed.
  # The commands will be chained like this: <before> && /setup && <after>
  # setupHook:
  #   before: echo This is a command before setup will executed
  #   after: echo This is a command after setup is executed

  # Dynamic is the default here. If you are more experience user then change these values as you like.
  # fpm:
  #   processManagement: dynamic
  #   maxChildren: 8
  #   startServers: 8
  #   minSpareServiers: 4
  #   maxSpareServiers: 8

  # horizontalPodAutoscaler:
  #   enabled: false
  #   minReplicas: 1
  #   maxReplicas: 10

minio:
  # This enabled the s3 MinIO and creates a tenant. This requires the MinIO operator
  # to work properly. You can use aws s3 also, but then need to overwrite the values
  # in the store resource.
  enabled: true
  # This disables the MinIO mtls. Please don't use this in production! This is required
  # for a local or Istio installation.
  useTLS: false
  # This enabled readonly for anonymous on the shopware-public bucket. This is required
  # for Shopware to work properly. But you can disable it. This will trigger a job in Kubernetes.
  publishPublicBucket: true
  tenantVersion: quay.io/minio/minio:RELEASE.2024-04-06T05-26-02Z
  # storageClassName: standard
  # storageSize: 10Gi
  # This image is used to add permissions for the public bucket.
  # When using Istio, the image must include curl; hence, we use minio/mc.
  # For non-Istio installations, the client image can be minio/mc.
  # clientImage: minio/mc

  # This configuration sets the username and password for MinIO login.
  # If the password is not specified, it will be automatically generated.
  #rootUser: admin
  #rootPassword: <random-id>

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  ingress:
    enabled: true
    className: nginx
    # annotations:

opensearch:
  enabled: false

redissession:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisapp:
  enabled: true
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisqueue:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 2000m
  auth:
    enabled: false

# blackfire:
  # serverID: <your id>
  # serverToken: <your token>
  # image: blackfire/blackfire:2
  # port: 8307
  # resources:
  #   requests:
  #     memory: 100Mi
  #     cpu: 100m
  #   limits:
  #     memory: 100Mi
  #     cpu: 100m

# Percona has currently only AMD images, so we make sure with a mixed cluster that we
# use the AMD clusters.
pxc-operator:
  nodeSelector:
    kubernetes.io/arch: amd64

# This controls the Shopware operator helm chart which can be found here:
# TODO: Helm chart link
shopware-operator:
  # Disabling makes sense if you want to test the Operator itself
  enabled: true

kube-prometheus-stack:
  enabled: true
  grafana:
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            uid: prometheus
            url: http://{{ .Release.Name }}-prometheus-server.{{ .Release.Namespace }}.svc.cluster.local
  prometheus:
    prometheusSpec:
      logLevel: debug
      logFormat: json
      scrapeConfigNamespaceSelector:
        matchLabels:
          name: shopware-test-monitoring
  nodeExporter:
    enabled: false
  alertmanager:
    enabled: false
