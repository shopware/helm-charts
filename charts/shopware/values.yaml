# This is for more production and enterprise solution. To see a full
# example see `examples/values_istio.yaml`.
#useIstio: false

percona:
  # You can disable Percona and use your own database. Just modify the storage database
  # part in this file.
  # TODO: Not tested yet, database part is not variable
  enabled: true

  # The version for Percona to use. This is also the image version for the percona-xtradb-cluster
  version: 1.14.0
  # Allows only one database and allows to overwrite the topologySpreadConstraints
  allowUnsafeConfigurations: true

  database:
    version: "8.0"
    # These are the default values for the data
    # size: 1

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 2000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 2000m
    # annotations:
    # affinity:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 8Gi

    # We overwrite the topologySpreadConstraints for this to schedule only one database
    topologySpreadConstraints:
    # Currently Percona only supports amd64, so we set a node selector.
    # This could also be a problem for Mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

  # This is only useful if you want more performance since the sql-proxy is using connection-pooling.
  proxy:
    enabled: false

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # size: 1
    # annotations:
    # resources:
    #   requests:
    #     memory: 1Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 1Gi
    #     cpu: 1000m
    # topologySpreadConstraints:
    # affinity:
    # annotations:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 10Gi

    # Currently Percona only supports amd64, so we set a node selector. This could also be
    # a problem for our mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

serviceAccountAnnotations: {}

store:
  # We use traefik.me to resolve to 127.0.0.1
  # Enter your DNS name for the shop.
  # This will make sure that your sales-channel is set up correctly and the ingress is using the host address.
  host: localhost.traefik.me

  # This configuration is used to disable S3 and database checks prior to setup and shop creation.
  disableChecks: false
  disableDatabaseCheck: false
  disableS3Check: false

  # The name of the service account to use for the store deployments.
  # remove this if you do not want to use a service account
  serviceAccountName: store-sa

  cdnURL: "example.com"

  # Shop settings can be configured at any time using the deployment helper.
  # For more information, refer to the documentation:
  # https://developer.shopware.com/docs/guides/hosting/installation-updates/deployments/deployment-helper.html
  # shopConfiguration:
  #   currency: EUR
  #   locale: en-GB

  # s3 storage configuration
  # s3Storage:
  #   privateBucket: <s3-private-bucket>
  #   publicBucket: <s3-public-bucket>
  #   region: <s3-region>
  #   endpointURL: <s3-endpoint-url>
  #   # Specify the credentials for the S3 storage. These are used to connect to the S3 storage.
  #   # Otherwise annotate the service account with a role
  #   accessKeyRef:
  #     name: s3-credentials
  #     key: AWS_ACCESS_KEY
  #   secretAccessKeyRef:
  #     name: s3-credentials
  #     key: AWS_SECRET_KEY

  # adminCredentials:
  #   username: admin
  #   password: <if empty it will be generated>

  # blackfire:
  #   enabled: false
  #   host: blackfire
  #   port: 8707

  # If you want to use tracing with otel make sure to disable blackfire.
  # It is not possible to use both.
  # otel:
  #   enabled: false
  #   exporterEndpoint: http://opentelemetry-collector.opentelemetry-collector.svc.cluster.local:4317

  database:
    user: "root"
    # version: '8'
    # port: 3306
    # name: 'shopware'

    # You need to set a host or a hostRef for the database to work. Otherwise the shopware Operator will
    # return an error. Default is the host of percona if percona is enabled.
    # If you don't want to use percona you need to specify the host or hostRef and the passwordSecretRef.
    # This will be set automatically if you use percona.
    #host: ""
    # hostRef:
    #   name: some-secret-name
    #   key: host-key
    # passwordSecretRef:
    #   name: percona-secrets
    #   key: root

  # You can define a Monolog configuration here.
  # Please refer to the official Monolog documentation for details.
  # This configuration will be mounted in the deployment as a ConfigMap.
  # The configuration file is located at /var/www/html/config/packages/monolog.yaml
  # monolog:
  #   handlers:
  #     main:
  #       type: fingers_crossed
  #       action_level: info
  #       handler: nested
  #       excluded_http_codes: [404, 405]
  #       buffer_size: 50
  #     nested:
  #       type: group
  #       members: [stderr, file_log]
  #     stderr:
  #       type: stream
  #       path: php://stderr
  #       level: info
  #     file_log:
  #       type: stream
  #       path: "/var/log/shopware.log"
  #       level: info
  #       formatter: 'monolog.formatter.json'
  #     console:
  #       type: console
  #       process_psr_3_messages: false
  #       channels: ["!event", "!doctrine"]
  #     business_event_handler_buffer:
  #       level: info

  container:
    # This image is used for the setup job and the deployment. If you want to migrate to a new
    # version just update this and the operator makes sure to handle the migrations.
    image: ghcr.io/shopware/shopware-kubernetes:latest
    # imagePullPolicy: IfNotPresent
    # restartPolicy: Always
    # replicas: 2
    # progressDeadlineSeconds: 30
    # topologySpreadConstraints:
    # affinity:
    # nodeSelector:
    # annotations:
    # terminationGracePeriodSeconds: 30
    # labels:
    # imagePullSecrets:
    #   - name: regcred
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 1000m
    #

    # You can add side-car containers as you wish. Note that the PHP fpm exporter is
    # already attached, if you are not using `dynamic` in the fpm configuration.
    # extraContainers:
    #   - name: some-side-pod
    #     env:
    #     - name: PHP_FPM_SCRAPE_URI
    #       value: tcp://127.0.0.1:9000/status
    #     image: hipages/php-fpm_exporter
    #     imagePullPolicy: IfNotPresent
    #     ports:
    #     - containerPort: 9253
    #       protocol: TCP
    #     resources:
    #       limits:
    #         cpu: 30m
    #         memory: 32Mi
    #       requests:
    #         cpu: 10m
    #         memory: 10Mi
    #     terminationMessagePath: /dev/termination-log
    #     terminationMessagePolicy: File

    # Warning: You can overwrite ENVs provided by the operator. If a variable gets overwritten
    # The operator prints a warning in the logs.
    # extraEnvs:
    #   - name: ENVS
    #     value: SomeStringValue
    #   - name: FPM_PM
    #     value: static

  # Each deployment and job from the operator can be overridden using the default container specifications defined
  # in the store's container specification.
  # migrationJobContainer:
  # setupJobContainer:
  # adminDeploymentContainer:
  # storefrontDeploymentContainer:
  # workerDeploymentContainer:

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  network:
    enabled: true
    ingressClassName: nginx
    # annotations:

  # You have the option to run commands both before and after the /setup command is executed.
  # If you wish to run the default setup script included in the official Shopware Docker image,
  # ensure that the /setup command is invoked.
  # In case the /setup command fails, it is crucial to exit the process with an appropriate
  # error code. Failing to provide an error code will result in the job being marked as
  # successful, even though the store may be in a non-functional state.
  # setupScript: |
  #   echo "Execute setup script"
  #   /setup
  #   if [ $? -eq 0 ]; then
  #       echo "Setup Finished"
  #   else
  #       echo "Setup Failed: exit with 20"
  #       exit 20
  #   fi

  # You can also change the migration script in the same format as the setup script
  # The default is /setup
  # migrationScript: /setup

  # Dynamic is the default here. If you are more experience user then change these values as you like.
  # fpm:
  #   processManagement: dynamic
  #   maxChildren: 8
  #   startServers: 8
  #   minSpareServiers: 4
  #   maxSpareServiers: 8

  # horizontalPodAutoscaler:
  #   enabled: false
  #   minReplicas: 1
  #   maxReplicas: 10

  # You can overwrite the default store.container values for each deployment and job. This is useful if you want to
  # add labels or other values to the containers.
  # adminDeploymentContainer:
  #   labels:
  #     test: 'adminContainer'
  #
  # workerDeploymentContainer:
  #   labels:
  #     test: 'workerContainer'
  #
  # storefrontDeploymentContainer:
  #   labels:
  #     test: 'storefrontContainer'
  #
  # setupJobContainer:
  #   labels:
  #     test: 'setupContainer'
  #
  # migrationJobContainer:
  #   labels:
  #     test: 'migrationContainer'

minio:
  # This enabled the s3 MinIO and creates a tenant. This requires the MinIO operator
  # to work properly. You can use aws s3 also, but then need to overwrite the values
  # in the store resource.
  enabled: true
  # This disables the MinIO mtls. Please don't use this in production! This is required
  # for a local or Istio installation.
  useTLS: false
  # This enabled readonly for anonymous on the shopware-public bucket. This is required
  # for Shopware to work properly. But you can disable it. This will trigger a job in Kubernetes.
  publishPublicBucket: true
  tenantVersion: quay.io/minio/minio:RELEASE.2024-04-06T05-26-02Z
  # storageClassName:
  # storageSize: 10Gi
  # This image is used to add permissions for the public bucket.
  # When using Istio, the image must include curl; hence, we use minio/mc.
  # For non-Istio installations, the client image can be minio/mc.
  # clientImage: minio/mc
  # resources:
  #   requests:
  #     memory: 2Gi
  #     cpu: 1000m
  #   limits:
  #     memory: 2Gi
  #     cpu: 1000m

  # This configuration sets the username and password for MinIO login.
  # If the password is not specified, it will be automatically generated.
  #rootUser: admin
  #rootPassword: <random-id>

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  ingress:
    enabled: true
    className: nginx
    # annotations:
    tls: {}

opensearch:
  enabled: false

valkeyapp:
  enabled: true
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

valkeysession:
  enabled: true
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

valkeyworker:
  enabled: true
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 2000m
  auth:
    enabled: false

redisapp:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 1000m
  # auth:
  #   enabled: false

redissession:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 1000m
  # auth:
  #   enabled: false

redisworker:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 2000m
  # auth:
  #   enabled: false

# blackfire:
# serverID: <your id>
# serverToken: <your token>
# image: blackfire/blackfire:2
# port: 8307
# resources:
#   requests:
#     memory: 100Mi
#     cpu: 100m
#   limits:
#     memory: 100Mi
#     cpu: 100m

# Percona has currently only AMD images, so we make sure with a mixed cluster that we
# use the AMD clusters.
pxc-operator:
  nodeSelector:
    kubernetes.io/arch: amd64

# This controls the Shopware operator helm chart which can be found here:
# DEPRECATED: This has moved because the seperated crd installation.
# shopware-operator:
#   # Disabling makes sense if you want to test the Operator itself
#   enabled: true
#   crds:
#     install: true

otel-collector:
  enabled: false
  image:
    repository: "otel/opentelemetry-collector-contrib"
    tag: "0.111.0"
  mode: deployment
  resources:
    requests:
      memory: "256Mi"
      cpu: "64m"
    limits:
      memory: "512Mi"
  config:
    exporters:
      debug: {}
      otlp:
        headers:
          x-scope-orgid: "tenant-{{ .Release.Namespace }}"
        endpoint: tempo.tempo.svc.cluster.local:4317
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch:
        timeout: 5s
        send_batch_size: 1048
      filter/ottl:
        error_mode: ignore
        traces:
          span:
            - 'name == "GET /api/_info/health-check"'
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${env:MY_POD_IP}:8888
    service:
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
      extensions:
        - health_check
      pipelines:
        logs:
          exporters:
            - debug
            - otlp
          processors:
            - batch
            - filter/ottl
          receivers:
            - otlp
        metrics:
          exporters:
            - debug
          processors:
            - batch
          receivers:
            - otlp
            - prometheus
        traces:
          exporters:
            - debug
            - otlp
          processors:
            - batch
            - filter/ottl
          receivers:
            - otlp
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
      # nodePort: 30317
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

grafana:
  enabled: false
  podAnnotations:
    sidecar.istio.io/inject: "false"
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://{{ .Release.Name }}-prometheus-server.{{ .Release.Namespace }}.svc.cluster.local
        - name: Loki
          type: loki
          url: http://loki-gateway.loki.svc.cluster.local
          jsonData:
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            httpHeaderValue1: tenant-{{ .Release.Namespace }}
        - name: Tempo
          type: tempo
          url: http://tempo.tempo.svc.cluster.local
          jsonData:
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            httpHeaderValue1: tenant-{{ .Release.Namespace }}

prometheus:
  enabled: false
  prometheus-node-exporter:
    enabled: false
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  # This configuration excludes Prometheus from scraping the Kubernetes API.
  # To collect additional metrics beyond those provided by Shopware, either uncomment this section or apply a different configuration.
  rbac:
    create: true
  server:
    configMapOverrideName: "prometheus-config"
    releaseNamespace: true
