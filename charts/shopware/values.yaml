# This is for more production and enterprise solution. To see a full
# example see `examples/values_istio.yaml`.
#useIstio: false

percona:
  # You can disable Percona and use your own database. Just modify the storage database
  # part in this file.
  # TODO: Not tested yet, database part is not variable
  enabled: true

  # The version for Percona to use. This is also the image version for the percona-xtradb-cluster
  version: 1.14.0
  # Allows only one database and allows to overwrite the topologySpreadConstraints
  allowUnsafeConfigurations: true

  database:
    version: "8.0"
    # These are the default values for the data
    # size: 1

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 2000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 2000m
    # annotations:
    # affinity:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 8Gi

    # We overwrite the topologySpreadConstraints for this to schedule only one database
    topologySpreadConstraints:

  # This is only useful if you want more performance since the sql-proxy is using connection-pooling.
  proxy:
    enabled: false

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # size: 1
    # annotations:
    # resources:
    #   requests:
    #     memory: 1Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 1Gi
    #     cpu: 1000m
    # topologySpreadConstraints:
    # affinity:
    # annotations:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 10Gi

serviceAccountAnnotations: {}

store:
  # DEPRECATED, please use appURLHost in the network settings. With the next miner version this setting will be gone.
  # The host will overwrite the network.appURLHost. Check appURLHost for more information.
  # host: localhost.traefik.me

  # annotations:
  # labels:

  # This configuration is used to disable S3 and database checks prior to setup and shop creation.
  disableChecks: false
  disableDatabaseCheck: false
  disableS3Check: false

  # The name of the service account to use for the store deployments.
  # remove this if you do not want to use a service account
  serviceAccountName: store-sa
  # Disables mounting the kubernetes service account token
  automountServiceAccountToken: false

  caddy:
    # This is the read timeout for the caddy server. Default is 60s.
    readTimeout: 60s
    # This is the write timeout for the caddy server. Default is 60s.
    writeTimeout: 60s

  cdnURL: "example.com"

  # Shop settings can be configured at any time using the deployment helper.
  # For more information, refer to the documentation:
  # https://developer.shopware.com/docs/guides/hosting/installation-updates/deployments/deployment-helper.html
  # shopConfiguration:
  #   currency: EUR
  #   locale: en-GB

  # s3 storage configuration
  # s3Storage:
  #   privateBucket: <s3-private-bucket>
  #   publicBucket: <s3-public-bucket>
  #   region: <s3-region>
  #   endpointURL: <s3-endpoint-url>
  #   # Specify the credentials for the S3 storage. These are used to connect to the S3 storage.
  #   # Otherwise annotate the service account with a role
  #   accessKeyRef:
  #     name: s3-credentials
  #     key: AWS_ACCESS_KEY
  #   secretAccessKeyRef:
  #     name: s3-credentials
  #     key: AWS_SECRET_KEY

  # adminCredentials:
  #   username: admin
  #   password: <if empty it will be generated>

  # blackfire:
  #   enabled: false
  #   host: blackfire
  #   port: 8707

  # If you want to use tracing with otel make sure to disable blackfire.
  # It is not possible to use both.
  # otel:
  #   enabled: false
  #   exporterEndpoint: http://opentelemetry-collector.opentelemetry-collector.svc.cluster.local:4317

  database:
    user: "root"
    # version: '8'
    # port: 3306
    # name: 'shopware'

    # You need to set a host or a hostRef for the database to work. Otherwise the shopware Operator will
    # return an error. Default is the host of percona if percona is enabled.
    # If you don't want to use percona you need to specify the host or hostRef and the passwordSecretRef.
    # This will be set automatically if you use percona.
    #host: ""
    # hostRef:
    #   name: some-secret-name
    #   key: host-key
    # passwordSecretRef:
    #   name: percona-secrets
    #   key: root

  # You can define a Monolog configuration here.
  # Please refer to the official Monolog documentation for details.
  # This configuration will be mounted in the deployment as a ConfigMap.
  # The configuration file is located at /var/www/html/config/packages/monolog.yaml
  # monolog:
  #   handlers:
  #     main:
  #       type: fingers_crossed
  #       action_level: info
  #       handler: nested
  #       excluded_http_codes: [404, 405]
  #       buffer_size: 50
  #     nested:
  #       type: group
  #       members: [stderr, file_log]
  #     stderr:
  #       type: stream
  #       path: php://stderr
  #       level: info
  #     file_log:
  #       type: stream
  #       path: "/var/log/shopware.log"
  #       level: info
  #       formatter: 'monolog.formatter.json'
  #     console:
  #       type: console
  #       process_psr_3_messages: false
  #       channels: ["!event", "!doctrine"]
  #     business_event_handler_buffer:
  #       level: info

  container:
    # This image is used for the setup job and the deployment. If you want to migrate to a new
    # version just update this and the operator makes sure to handle the migrations.
    image: ghcr.io/shopware/shopware-kubernetes:latest
    # imagePullPolicy: IfNotPresent
    # restartPolicy: Always
    # replicas: 2
    # progressDeadlineSeconds: 30
    # topologySpreadConstraints:
    # affinity:
    # nodeSelector:
    # annotations:
    # terminationGracePeriodSeconds: 30
    # labels:
    # imagePullSecrets:
    #   - name: regcred
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 1000m
    #

    # You can add side-car containers as you wish. Note that the PHP fpm exporter is
    # already attached, if you are not using `dynamic` in the fpm configuration.
    # extraContainers:
    #   - name: some-side-pod
    #     env:
    #     - name: PHP_FPM_SCRAPE_URI
    #       value: tcp://127.0.0.1:9000/status
    #     image: hipages/php-fpm_exporter
    #     imagePullPolicy: IfNotPresent
    #     ports:
    #     - containerPort: 9253
    #       protocol: TCP
    #     resources:
    #       limits:
    #         cpu: 30m
    #         memory: 32Mi
    #       requests:
    #         cpu: 10m
    #         memory: 10Mi
    #     terminationMessagePath: /dev/termination-log
    #     terminationMessagePolicy: File

    # Warning: You can overwrite ENVs provided by the operator. If a variable gets overwritten
    # The operator prints a warning in the logs.
    # extraEnvs:
    #   - name: ENVS
    #     value: SomeStringValue
    #   - name: FPM_PM
    #     value: static

  #scheduledTask:
  #  command: bin/console scheduled-task:run -v -n --no-wait
  #   suspend: false
  #   # The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.
  #   schedule: 0 * * * *
  #   timeZine: Etc/UTC

  # Each deployment and job from the operator can be overridden using the default container specifications defined
  # in the store's container specification.
  # migrationJobContainer:
  # setupJobContainer:
  # adminDeploymentContainer:
  # storefrontDeploymentContainer:
  # workerDeploymentContainer:

  # OpenSearch configuration for product search and indexing
  # When enabled, host and passwordSecretRef are required
  opensearch:
    enabled: false
    # host: opensearch.local
    # username: admin
    # passwordSecretRef:
    #   name: opensearch-credentials
    #   key: password
    # port: 9200
    # schema: https
    # index:
    #   prefix: sw
    #   shards: 3
    #   replicas: 3

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  network:
    # We use traefik.me to resolve to 127.0.0.1
    # Enter your DNS name for the shop.
    # Sets the ingress rules for each host.
    hosts:
      - localhost.traefik.me

    # This is the URL where the SBP is pinging for a license check. The URL must be also in the the hosts list.
    # This will make sure that your sales-channel is set up correctly. Passed to shopware as env APP_URL
    # Supported with shopware-operator:0.0.45
    appURLHost: localhost.traefik.me

    enabledIngress: true
    ingressClassName: nginx
    # ingressAnnotations:
    # ingressLabels:

    # DEPRECATED: Works only for nginx when used
    # annotations:
    # DEPRECATED: Works only for nginx when used
    # enabled: true

    # enabledGateway: false
    # gatewayName:
    # gatewaySectionName:
    # httpRouteAnnotations:
    # httpRouteLabels:

  # You have the option to run commands both before and after the /setup command is executed.
  # If you wish to run the default setup script included in the official Shopware Docker image,
  # ensure that the /setup command is invoked.
  # In case the /setup command fails, it is crucial to exit the process with an appropriate
  # error code. Failing to provide an error code will result in the job being marked as
  # successful, even though the store may be in a non-functional state.
  # setupScript: |
  #   echo "Execute setup script"
  #   /setup
  #   if [ $? -eq 0 ]; then
  #       echo "Setup Finished"
  #   else
  #       echo "Setup Failed: exit with 20"
  #       exit 20
  #   fi

  # You can also change the migration script in the same format as the setup script
  # The default is /setup
  # migrationScript: /setup

  # Dynamic is the default here. If you are more experience user then change these values as you like.
  # fpm:
  #   processManagement: dynamic
  #   maxChildren: 8
  #   startServers: 8
  #   minSpareServiers: 4
  #   maxSpareServiers: 8

  # horizontalPodAutoscaler:
  #   enabled: false
  #   minReplicas: 1
  #   maxReplicas: 10

  # You can overwrite the default store.container values for each deployment and job. This is useful if you want to
  # add labels or other values to the containers.
  # adminDeploymentContainer:
  #   labels:
  #     test: 'adminContainer'
  #
  # workerDeploymentContainer:
  #   labels:
  #     test: 'workerContainer'
  #
  # storefrontDeploymentContainer:
  #   labels:
  #     test: 'storefrontContainer'
  #
  # setupJobContainer:
  #   labels:
  #     test: 'setupContainer'
  #
  # migrationJobContainer:
  #   labels:
  #     test: 'migrationContainer'

rustfs:
  # This enables RustFS, a high-performance S3-compatible object store written in Rust.
  # RustFS is used for public assets and private files, providing an alternative to MinIO.
  enabled: true

  # RustFS supports two deployment modes:
  # - standalone (default): Single pod with single PVC for development/testing
  # - distributed: Multiple pods with multiple PVCs for high availability
  mode:
    distributed:
      enabled: false
    standalone:
      enabled: true

  # Number of RustFS nodes (only applicable in distributed mode)
  replicaCount: 1

  # Storage configuration
  storageclass:
    name: "standard"
    dataStorageSize: 256Mi
    logStorageSize: 256Mi

  # RustFS credentials
  # If not specified, defaults will be used (rustfsadmin/rustfsadmin)
  # secret:
  #   rustfs:
  #     access_key: rustfsadmin
  #     secret_key: rustfsadmin

  # Ingress configuration
  # You need to set the correct ingressClassName for this to work properly.
  # If you follow the Readme and use kind, you can use nginx as ingress class.
  # The rustfs chart creates an ingress for the console (port 9001) using these settings
  ingress:
    enabled: true
    className: nginx
    hosts:
      - host: s3-console.traefik.me
        paths:
          - path: /
            pathType: ImplementationSpecific
    nginxAnnotations:
      nginx.ingress.kubernetes.io/affinity: cookie
      nginx.ingress.kubernetes.io/session-cookie-expires: "3600"
      nginx.ingress.kubernetes.io/session-cookie-hash: sha1
      nginx.ingress.kubernetes.io/session-cookie-max-age: "3600"
      nginx.ingress.kubernetes.io/session-cookie-name: rustfs
    # tls: []

  # S3 API endpoint ingress (port 9000)
  # This is managed by the Shopware chart, not the rustfs subchart
  s3Ingress:
    enabled: true
    className: nginx
    host: s3.traefik.me
    annotations:
      nginx.ingress.kubernetes.io/affinity: cookie
      nginx.ingress.kubernetes.io/session-cookie-expires: "3600"
      nginx.ingress.kubernetes.io/session-cookie-hash: sha1
      nginx.ingress.kubernetes.io/session-cookie-max-age: "3600"
      nginx.ingress.kubernetes.io/session-cookie-name: rustfs
    # tls: []

valkeyapp:
  enabled: true
  image:
    registry: docker.io
    repository: valkey/valkey
    tag: 8.0
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

valkeysession:
  enabled: true
  image:
    registry: docker.io
    repository: valkey/valkey
    tag: 8.0
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

valkeyworker:
  enabled: true
  image:
    registry: docker.io
    repository: valkey/valkey
    tag: 8.0
  architecture: standalone
  replica:
    replicaCount: 1
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 2000m
  auth:
    enabled: false

redisapp:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 1000m
  # auth:
  #   enabled: false

redissession:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 1000m
  # auth:
  #   enabled: false

redisworker:
  enabled: false
  # architecture: standalone
  # replica:
  #   replicaCount: 1
  # master:
  #   resources:
  #     requests:
  #       memory: 500Mi
  #       cpu: 1000m
  #     limits:
  #       memory: 1Gi
  #       cpu: 2000m
  # auth:
  #   enabled: false

# blackfire:
# serverID: <your id>
# serverToken: <your token>
# image: blackfire/blackfire:2
# port: 8307
# resources:
#   requests:
#     memory: 100Mi
#     cpu: 100m
#   limits:
#     memory: 100Mi
#     cpu: 100m

# This controls the Shopware operator helm chart which can be found here:
# DEPRECATED: This has moved because the seperated crd installation.
# shopware-operator:
#   # Disabling makes sense if you want to test the Operator itself
#   enabled: true
#   crds:
#     install: true

otel-collector:
  enabled: false
  image:
    repository: "otel/opentelemetry-collector-contrib"
    tag: "0.111.0"
  mode: deployment
  resources:
    requests:
      memory: "256Mi"
      cpu: "64m"
    limits:
      memory: "512Mi"
  config:
    exporters:
      debug: {}
      otlp:
        headers:
          x-scope-orgid: "tenant-{{ .Release.Namespace }}"
        endpoint: tempo.tempo.svc.cluster.local:4317
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch:
        timeout: 5s
        send_batch_size: 1048
      filter/ottl:
        error_mode: ignore
        traces:
          span:
            - 'name == "GET /api/_info/health-check"'
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${env:MY_POD_IP}:8888
    service:
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
      extensions:
        - health_check
      pipelines:
        logs:
          exporters:
            - debug
            - otlp
          processors:
            - batch
            - filter/ottl
          receivers:
            - otlp
        metrics:
          exporters:
            - debug
          processors:
            - batch
          receivers:
            - otlp
            - prometheus
        traces:
          exporters:
            - debug
            - otlp
          processors:
            - batch
            - filter/ottl
          receivers:
            - otlp
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
      # nodePort: 30317
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

grafana:
  enabled: false
  podAnnotations:
    sidecar.istio.io/inject: "false"
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://{{ .Release.Name }}-prometheus-server.{{ .Release.Namespace }}.svc.cluster.local
        - name: Loki
          type: loki
          url: http://loki-gateway.loki.svc.cluster.local
          jsonData:
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            httpHeaderValue1: tenant-{{ .Release.Namespace }}
        - name: Tempo
          type: tempo
          url: http://tempo.tempo.svc.cluster.local
          jsonData:
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            httpHeaderValue1: tenant-{{ .Release.Namespace }}

prometheus:
  enabled: false
  prometheus-node-exporter:
    enabled: false
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  # This configuration excludes Prometheus from scraping the Kubernetes API.
  # To collect additional metrics beyond those provided by Shopware, either uncomment this section or apply a different configuration.
  rbac:
    create: true
  server:
    configMapOverrideName: "prometheus-config"
    releaseNamespace: true
# additional kubernetes objects
# extraObjects:
#   - apiVersion: v1
#     kind: ConfigMap
#     metadata:
#       name: s3-fastly-connection
#     data:
#       domains: |
#         - cdn.foo.example
#       port: "443"
#       address: <bucket>.s3.eu-central-1.amazonaws.com
