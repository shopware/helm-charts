# This is for more production and enterprise solution. To see a full
# example see `examples/values_istio.yaml`.
#useIstio: false

percona:
  # You can disable Percona and use your own database. Just modify the storage database
  # part in this file.
  # TODO: Not tested yet, database part is not variable
  enabled: true

  # The version for Percona to use. This is also the image version for the percona-xtradb-cluster
  version: 1.14.0
  # Allows only one database and allows to overwrite the topologySpreadConstraints
  allowUnsafeConfigurations: true

  database:
    version: "8.0"
    # These are the default values for the data
    # size: 1

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 2000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 2000m
    # annotations:
    # affinity:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 8Gi

    # We overwrite the topologySpreadConstraints for this to schedule only one database
    topologySpreadConstraints:
    # Currently Percona only supports amd64, so we set a node selector.
    # This could also be a problem for Mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

  # This is only useful if you want more performance since the sql-proxy is using connection-pooling.
  proxy:
    enabled: false

    # You can overwrite these default values from Percona. To see which values are valid check
    # the Percona documentation on this: https://github.com/percona/percona-helm-charts/blob/main/charts/pxc-db/values.yaml
    # If there are values missing for you, add them to the template and make a PR.
    # size: 1
    # annotations:
    # resources:
    #   requests:
    #     memory: 1Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 1Gi
    #     cpu: 1000m
    # topologySpreadConstraints:
    # affinity:
    # annotations:
    # serviceAnnotations:
    # storageClassName:
    # storageSize: 10Gi

    # Currently Percona only supports amd64, so we set a node selector. This could also be
    # a problem for our mac users here.
    # TODO: Change the image once they support AMD and ARM architectures
    nodeSelector:
      kubernetes.io/arch: amd64

store:
  # We use traefik.me to resolve to 127.0.0.1
  # Enter your DNS name for the shop.
  # This will make sure that your sales-channel is set up correctly and the ingress is using the host address.
  # host: localhost.traefik.me

  # This configuration is used to disable S3 and database checks prior to setup and shop creation.
  # Note: This is a known issue in the operator and will be addressed in a future update.
  # For more details, see: https://github.com/shopware/shopware-operator/issues/20
  disableChecks: true

  # adminCredentials:
  #   username: admin
  #   password: <if empty it will be generated>

  # blackfire:
  #   enabled: false
  #   host: blackfire
  #   port: 8707

  # If you want to use tracing with otel make sure to disable blackfire.
  # It is not possible to use both.
  # otel:
  #   enabled: false
  #   exporterEndpoint: http://opentelemetry-collector.opentelemetry-collector.svc.cluster.local:4317

  # You can define a Monolog configuration here.
  # Please refer to the official Monolog documentation for details.
  # This configuration will be mounted in the deployment as a ConfigMap.
  # The configuration file is located at /var/www/html/config/packages/monolog.yaml
  # monolog:
  #   handlers:
  #     main:
  #       type: fingers_crossed
  #       action_level: info
  #       handler: nested
  #       excluded_http_codes: [404, 405]
  #       buffer_size: 50
  #     nested:
  #       type: group
  #       members: [stderr, file_log]
  #     stderr:
  #       type: stream
  #       path: php://stderr
  #       level: info
  #     file_log:
  #       type: stream
  #       path: "/var/log/shopware.log"
  #       level: info
  #       formatter: 'monolog.formatter.json'
  #     console:
  #       type: console
  #       process_psr_3_messages: false
  #       channels: ["!event", "!doctrine"]
  #     business_event_handler_buffer:
  #       level: info

  container:
    # This image is used for the setup job and the deployment. If you want to migrate to a new
    # version just update this and the operator makes sure to handle the migrations.
    image: ghcr.io/shopware/shopware-kubernetes:latest
    # imagePullPolicy: IfNotPresent
    # restartPolicy: Always
    # replicas: 2
    # progressDeadlineSeconds: 30
    # topologySpreadConstraints:
    # affinity:
    # nodeSelector:
    # annotations:
    # imagePullSecrets:
    #   - name: regcred
    # resources:
    #   requests:
    #     memory: 2Gi
    #     cpu: 1000m
    #   limits:
    #     memory: 2Gi
    #     cpu: 1000m
    #

    # You can add side-car containers as you wish. Note that the PHP fpm exporter is
    # already attached, if you are not using `dynamic` in the fpm configuration.
    # extraContainers:
    #   - name: some-side-pod
    #     env:
    #     - name: PHP_FPM_SCRAPE_URI
    #       value: tcp://127.0.0.1:9000/status
    #     image: hipages/php-fpm_exporter
    #     imagePullPolicy: IfNotPresent
    #     ports:
    #     - containerPort: 9253
    #       protocol: TCP
    #     resources:
    #       limits:
    #         cpu: 30m
    #         memory: 32Mi
    #       requests:
    #         cpu: 10m
    #         memory: 10Mi
    #     terminationMessagePath: /dev/termination-log
    #     terminationMessagePolicy: File

    # Warning: You can overwrite ENVs provided by the operator. If a variable gets overwritten
    # The operator prints a warning in the logs.
    # extraEnvs:
    #   - name: ENVS
    #     value: SomeStringValue
    #   - name: FPM_PM
    #     value: static

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  network:
    enabled: true
    ingressClassName: nginx
    # annotations:

  # You can run commands before and after the /setup is executed.
  # The commands will be chained like this: <before> && /setup && <after>
  # setupHook:
  #   before: echo This is a command before setup will executed
  #   after: echo This is a command after setup is executed

  # Dynamic is the default here. If you are more experience user then change these values as you like.
  # fpm:
  #   processManagement: dynamic
  #   maxChildren: 8
  #   startServers: 8
  #   minSpareServiers: 4
  #   maxSpareServiers: 8

  # horizontalPodAutoscaler:
  #   enabled: false
  #   minReplicas: 1
  #   maxReplicas: 10

minio:
  # This enabled the s3 MinIO and creates a tenant. This requires the MinIO operator
  # to work properly. You can use aws s3 also, but then need to overwrite the values
  # in the store resource.
  enabled: true
  # This disables the MinIO mtls. Please don't use this in production! This is required
  # for a local or Istio installation.
  useTLS: false
  # This enabled readonly for anonymous on the shopware-public bucket. This is required
  # for Shopware to work properly. But you can disable it. This will trigger a job in Kubernetes.
  publishPublicBucket: true
  tenantVersion: quay.io/minio/minio:RELEASE.2024-04-06T05-26-02Z
  # storageClassName:
  # storageSize: 10Gi
  # This image is used to add permissions for the public bucket.
  # When using Istio, the image must include curl; hence, we use minio/mc.
  # For non-Istio installations, the client image can be minio/mc.
  # clientImage: minio/mc

  # This configuration sets the username and password for MinIO login.
  # If the password is not specified, it will be automatically generated.
  #rootUser: admin
  #rootPassword: <random-id>

  # You need to set the correct ingressClassName for this to work properly. If you follow
  # the Readme and use kind, you can use nginx as ingress class and everything should run
  # out of the box.
  ingress:
    enabled: true
    className: nginx
    # annotations:

opensearch:
  enabled: false

redissession:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisapp:
  enabled: true
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 1000m
  auth:
    enabled: false

redisqueue:
  enabled: false
  architecture: standalone
  master:
    resources:
      requests:
        memory: 500Mi
        cpu: 1000m
      limits:
        memory: 1Gi
        cpu: 2000m
  auth:
    enabled: false

# blackfire:
  # serverID: <your id>
  # serverToken: <your token>
  # image: blackfire/blackfire:2
  # port: 8307
  # resources:
  #   requests:
  #     memory: 100Mi
  #     cpu: 100m
  #   limits:
  #     memory: 100Mi
  #     cpu: 100m

# Percona has currently only AMD images, so we make sure with a mixed cluster that we
# use the AMD clusters.
pxc-operator:
  nodeSelector:
    kubernetes.io/arch: amd64

# This controls the Shopware operator helm chart which can be found here:
shopware-operator:
  # Disabling makes sense if you want to test the Operator itself
  enabled: true

grafana:
  enabled: true
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://{{ .Release.Namespace }}-prometheus-server.{{ .Release.Namespace }}.svc.cluster.local
        - name: Loki
          type: loki
          url: http://loki-gateway.loki.svc.cluster.local
          jsonData:
            httpHeaderName1: 'X-Scope-OrgID'
          secureJsonData:
            httpHeaderValue1: tenant-{{ .Release.Namespace }}

prometheus:
  enabled: true
  prometheus-node-exporter:
    enabled: false
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  # This configuration excludes Prometheus from scraping the Kubernetes API.
  # To collect additional metrics beyond those provided by Shopware, either uncomment this section or apply a different configuration.
  rbac:
    create: false
  server:
    configMapOverrideName: "prometheus-config"
    releaseNamespace: true
